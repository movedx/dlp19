{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Programming Exercise 7: Convolutional Networks in Keras\n",
    "\n",
    "Welcome to the 7th assignment of deep learning programming!\n",
    "In this assignment you will implement a convolutional neural network in Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import keras\n",
    "import h5py\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import *\n",
    "from utils.data_utils import load_CIFAR10\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "We will use the same dataset as previous assignments. \n",
    "However, note the CNN input is different from that of traditional neural networks, it is width x height x channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000, 10)\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000, 10)\n",
      "Test data shape:  (1000, 32, 32, 3)\n",
      "Test labels shape:  (1000, 10)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = '../../data/cifar'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Normalize the data\n",
    "    X_train /= 255\n",
    "    X_val /= 255\n",
    "    X_test /= 255\n",
    "    X_dev /= 255\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: build a model [10pt]\n",
    "\n",
    "This is an open assgiment, you should use whatever you have learnt in this course to build a promising model. For example, dropout, batch normalization, etc. After building model, you can call `model.summary()` to check architecture of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               524416    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 916,426\n",
      "Trainable params: 915,274\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (32, 32, 3)\n",
    "\n",
    "### START CODE HERE ###\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),padding='same',activation='relu',input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), padding='same',activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(128, (3, 3),padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(256, (3, 3),padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "### END CODE HERE ###\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: compile, train and test your model [2pt]\n",
    "\n",
    "After a number of epoches, you can achieve a much better performance than previous assginments. \n",
    "For example, our implementation has nearly 85% on valiation set and around 83% on test set.\n",
    "\n",
    "Hint: use `Callbacks` in Keras to keep the best model.\n",
    "\n",
    "Important: Store your model as model.hdf5 and upload it to the git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 49000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "49000/49000 [==============================] - 8s 170us/step - loss: 1.7483 - acc: 0.3994 - val_loss: 1.6044 - val_acc: 0.4630\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.46300, saving model to model.hdf5\n",
      "Epoch 2/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 1.2809 - acc: 0.5400 - val_loss: 1.6196 - val_acc: 0.4510\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.46300\n",
      "Epoch 3/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 1.1039 - acc: 0.6060 - val_loss: 1.1159 - val_acc: 0.6130\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.46300 to 0.61300, saving model to model.hdf5\n",
      "Epoch 4/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 0.9909 - acc: 0.6494 - val_loss: 1.0744 - val_acc: 0.6090\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.61300\n",
      "Epoch 5/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.9089 - acc: 0.6783 - val_loss: 0.8943 - val_acc: 0.6710\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.61300 to 0.67100, saving model to model.hdf5\n",
      "Epoch 6/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 0.8476 - acc: 0.7004 - val_loss: 0.9565 - val_acc: 0.6460\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.67100\n",
      "Epoch 7/50\n",
      "49000/49000 [==============================] - 8s 160us/step - loss: 0.7977 - acc: 0.7189 - val_loss: 1.0941 - val_acc: 0.6260\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.67100\n",
      "Epoch 8/50\n",
      "49000/49000 [==============================] - 8s 157us/step - loss: 0.7556 - acc: 0.7344 - val_loss: 0.7729 - val_acc: 0.7200\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.67100 to 0.72000, saving model to model.hdf5\n",
      "Epoch 9/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.7231 - acc: 0.7448 - val_loss: 0.8947 - val_acc: 0.6990\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.72000\n",
      "Epoch 10/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 0.6914 - acc: 0.7579 - val_loss: 1.3483 - val_acc: 0.5890\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.72000\n",
      "Epoch 11/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.6650 - acc: 0.7656 - val_loss: 0.7007 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.72000 to 0.76000, saving model to model.hdf5\n",
      "Epoch 12/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.6404 - acc: 0.7768 - val_loss: 0.7530 - val_acc: 0.7260\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76000\n",
      "Epoch 13/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.6143 - acc: 0.7851 - val_loss: 0.7592 - val_acc: 0.7330\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76000\n",
      "Epoch 14/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.5967 - acc: 0.7902 - val_loss: 0.6724 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.76000 to 0.76700, saving model to model.hdf5\n",
      "Epoch 15/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.5762 - acc: 0.8003 - val_loss: 0.7142 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76700\n",
      "Epoch 16/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.5586 - acc: 0.8037 - val_loss: 0.9878 - val_acc: 0.6850\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76700\n",
      "Epoch 17/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.5388 - acc: 0.8109 - val_loss: 0.6049 - val_acc: 0.7860\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.76700 to 0.78600, saving model to model.hdf5\n",
      "Epoch 18/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.5204 - acc: 0.8148 - val_loss: 0.7302 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.78600\n",
      "Epoch 19/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.5105 - acc: 0.8205 - val_loss: 0.6739 - val_acc: 0.7850\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.78600\n",
      "Epoch 20/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.4940 - acc: 0.8259 - val_loss: 0.5291 - val_acc: 0.8210\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.78600 to 0.82100, saving model to model.hdf5\n",
      "Epoch 21/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.4802 - acc: 0.8317 - val_loss: 0.5314 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.82100 to 0.83000, saving model to model.hdf5\n",
      "Epoch 22/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.4669 - acc: 0.8350 - val_loss: 0.7282 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.83000\n",
      "Epoch 23/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.4541 - acc: 0.8406 - val_loss: 0.5277 - val_acc: 0.8310\n",
      "\n",
      "Epoch 00023: val_acc improved from 0.83000 to 0.83100, saving model to model.hdf5\n",
      "Epoch 24/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.4399 - acc: 0.8437 - val_loss: 0.6542 - val_acc: 0.7850\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.83100\n",
      "Epoch 25/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.4287 - acc: 0.8475 - val_loss: 0.5724 - val_acc: 0.8120\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.83100\n",
      "Epoch 26/50\n",
      "49000/49000 [==============================] - 8s 160us/step - loss: 0.4165 - acc: 0.8523 - val_loss: 0.7848 - val_acc: 0.7550\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.83100\n",
      "Epoch 27/50\n",
      "49000/49000 [==============================] - 8s 160us/step - loss: 0.4084 - acc: 0.8570 - val_loss: 0.8902 - val_acc: 0.7400\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.83100\n",
      "Epoch 28/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.3961 - acc: 0.8621 - val_loss: 0.5199 - val_acc: 0.8310\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.83100\n",
      "Epoch 29/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.3917 - acc: 0.8616 - val_loss: 0.5185 - val_acc: 0.8280\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.83100\n",
      "Epoch 30/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 0.3784 - acc: 0.8659 - val_loss: 0.5477 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.83100\n",
      "Epoch 31/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 0.3659 - acc: 0.8706 - val_loss: 0.5656 - val_acc: 0.8210\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.83100\n",
      "Epoch 32/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.3650 - acc: 0.8710 - val_loss: 0.5391 - val_acc: 0.8210\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.83100\n",
      "Epoch 33/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.3548 - acc: 0.8751 - val_loss: 0.5616 - val_acc: 0.8110\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.83100\n",
      "Epoch 34/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.3457 - acc: 0.8774 - val_loss: 0.5929 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.83100\n",
      "Epoch 35/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.3356 - acc: 0.8807 - val_loss: 0.5545 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.83100\n",
      "Epoch 36/50\n",
      "49000/49000 [==============================] - 8s 160us/step - loss: 0.3266 - acc: 0.8847 - val_loss: 0.5081 - val_acc: 0.8320\n",
      "\n",
      "Epoch 00036: val_acc improved from 0.83100 to 0.83200, saving model to model.hdf5\n",
      "Epoch 37/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 0.3263 - acc: 0.8859 - val_loss: 0.6457 - val_acc: 0.7930\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.83200\n",
      "Epoch 38/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.3153 - acc: 0.8865 - val_loss: 0.5204 - val_acc: 0.8490\n",
      "\n",
      "Epoch 00038: val_acc improved from 0.83200 to 0.84900, saving model to model.hdf5\n",
      "Epoch 39/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 0.3086 - acc: 0.8894 - val_loss: 0.7823 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.84900\n",
      "Epoch 40/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 0.3012 - acc: 0.8921 - val_loss: 0.9287 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.84900\n",
      "Epoch 41/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.2978 - acc: 0.8932 - val_loss: 0.5492 - val_acc: 0.8280\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.84900\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.2883 - acc: 0.8966 - val_loss: 0.5955 - val_acc: 0.8130\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.84900\n",
      "Epoch 43/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.2851 - acc: 0.8975 - val_loss: 0.5202 - val_acc: 0.8410\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.84900\n",
      "Epoch 44/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 0.2778 - acc: 0.9001 - val_loss: 0.5652 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.84900\n",
      "Epoch 45/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.2739 - acc: 0.9018 - val_loss: 0.5605 - val_acc: 0.8310\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.84900\n",
      "Epoch 46/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.2656 - acc: 0.9047 - val_loss: 0.7617 - val_acc: 0.7850\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.84900\n",
      "Epoch 47/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.2649 - acc: 0.9062 - val_loss: 0.6132 - val_acc: 0.8170\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.84900\n",
      "Epoch 48/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 0.2629 - acc: 0.9065 - val_loss: 0.7790 - val_acc: 0.7780\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.84900\n",
      "Epoch 49/50\n",
      "49000/49000 [==============================] - 8s 159us/step - loss: 0.2545 - acc: 0.9096 - val_loss: 0.5157 - val_acc: 0.8510\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.84900 to 0.85100, saving model to model.hdf5\n",
      "Epoch 50/50\n",
      "49000/49000 [==============================] - 8s 158us/step - loss: 0.2501 - acc: 0.9108 - val_loss: 0.4837 - val_acc: 0.8590\n",
      "\n",
      "Epoch 00050: val_acc improved from 0.85100 to 0.85900, saving model to model.hdf5\n",
      "Test loss: 0.46062334537506106\n",
      "Test accuracy: 0.852\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "\n",
    "# save the best model only\n",
    "filepath ='model.hdf5'\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath, \n",
    "                                             monitor='val_acc', \n",
    "                                             verbose=1, \n",
    "                                             save_best_only=True, \n",
    "                                             mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=SGD(lr=0.1, momentum=0, decay=0, nesterov=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "model.load_weights(filepath)\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Description [3pt]\n",
    "\n",
    "Describe in just a few words what the inuition behind the layers types in your model is, why/how they improve the performance compared to vanilla cnns. Be specific about the performance metric: Improving accuracy is not the same as improving training speed. Be critical, point out drawbacks that you can think of."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
